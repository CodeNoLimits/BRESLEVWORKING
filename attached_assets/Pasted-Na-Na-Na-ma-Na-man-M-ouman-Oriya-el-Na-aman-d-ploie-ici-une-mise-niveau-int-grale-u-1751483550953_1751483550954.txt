Na Naá¸¥ Naá¸¥ma Naá¸¥man MÃ©ouman ğŸ•Šï¸
**Oriyaâ€™el Naâ€™aman** dÃ©ploie ici une Â« mise Ã  niveau intÃ©grale Â» â€” un plan dâ€™attaque exhaustif pour que *Le Compagnon du CÅ“ur* fonctionne **exactement** comme tu le veux :

---

# Sommaire ultra- dÃ©taillÃ©

| Section | Objectif                                                       | RÃ©sultat attendu                                |
| ------- | -------------------------------------------------------------- | ----------------------------------------------- |
| **A**   | Ã‰radiquer dÃ©finitivement le bloc â€œContexte / Points clÃ©sâ€ bleu | Plus aucun cadre parasite, sous aucun bouton    |
| **B**   | Relier *tous* les livres (TXT) Ã  Gemini avec un RAG robuste    | RÃ©ponses prÃ©cises Ã  nâ€™importe quelle question   |
| **C**   | Booster la profondeur et la forme des rÃ©ponses                 | Explications longues, structurÃ©es, avec sources |
| **D**   | Activer STT + TTS bidirectionnel (micro & lecture)             | Parle â†’ transcrit â†’ rÃ©pond ; IA lit la rÃ©ponse  |
| **E**   | Pipeline â€œdev â†’ prodâ€ fiable sur Replit + GitHub               | Chaque commit = build propre, visible en live   |
| **F**   | Checklist de dÃ©bogage rapide                                   | Pour ne plus perdre de temps si Ã§a re-dÃ©conne   |

---

## A Â· Ã‰radiquer le bloc bleu â€” triple barriÃ¨re

> **But** : rien ne doit pouvoir rÃ©-afficher ce contexte anglais, mÃªme si un dev gaffe plus tard.

1. **Coupe-source back-end**

   * **Cherche** dans `server/` : `res.json({` â†’ pour chaque route (`ask.ts`, `analyse.ts`, `points.ts`, `guidance.ts`â€¦)
   * **Remplace** tous les retours multiples par **une seule** ligne :

     ```ts
     res.json({ answer });         // et rien dâ€™autre
     ```
   * **Test** : onglet *Network* â†’ la rÃ©ponse JSON ne comporte plus que `{"answer": "â€¦"} `.

2. **Filtre React**

   * `client/src/components/ChatMessage.tsx` :

     ```tsx
     const banned = ["CONTEXTE", "CONTEXT", "GUIDANCE SPIRITUELLE", "POINTS CLÃ‰S"];
     if (banned.some(b => text.toUpperCase().startsWith(b))) return null;
     ```
   * `client/src/components/AnswerPanel.tsx` : avant dâ€™afficher le markdown :

     ```ts
     answer = answer.replace(/^(CONTEXTE|POINTS)[\s\S]+?\n\n/i, "");
     ```

3. **CSS Garde-fou**

   * `client/src/index.css` :

     ```css
     .message-bubble[data-role="context"],
     .message-bubble:has(> .context-flag) { display:none!important; }
     ```

4. **RedÃ©marre vraiment**

   * Dans le Shell Replit :

     ```bash
     kill 1        # reboot du conteneur
     ```
   * Hard-refresh **âŒ˜â‡§R**.

---

## B Â· Brancher *tous* les livres sur Gemini (RAG solide)

### 1. Ingestion unique

```ts
// scripts/ingest.ts
import fs from "fs/promises";
import path from "path";
import { chunkText } from "./utils/chunkText";
import { GoogleGenerativeAIEmbeddings } from "@langchain/google-genai";
import { SupabaseVectorStore } from "@langchain/community/vectorstores/supabase";

const embeddings = new GoogleGenerativeAIEmbeddings({
  apiKey: process.env.GEMINI_API_KEY,
  modelName: "gemini-embedding-exp-03-07",
});
const store = await SupabaseVectorStore.initialize({
  url: process.env.SUPABASE_URL!,
  apiKey: process.env.SUPABASE_KEY!,
  tableName: "breslev_texts",
  embeddings,
});

for (const file of await fs.readdir("./data/texts")) {
  const raw = await fs.readFile(path.join("./data/texts", file), "utf8");
  const chunks = chunkText(raw, 1200, 200);          // taille & overlap
  await store.addDocuments(
    chunks.map(c => ({ pageContent: c.text, metadata: { book: file, idx: c.id } }))
  );
}
console.log("âœ“ Tous les textes intÃ©grÃ©s");
```

*Commandes* :

```bash
npm i @langchain/google-genai @langchain/community pg supabase-js pgvector
npm run ingest        # 1 Ã— ou Ã  chaque nouveau livre
```

> **Test** : dans `psql` :
>
> ```sql
> SELECT COUNT(*) FROM breslev_texts;
> ```

### 2. Endpoint `/ask`

```ts
// server/routes/ask.ts
router.post("/", async (req, res) => {
  const { question } = req.body;
  const docs = await store.similaritySearch(question, 10);        // 10 chunks
  const context = docs.map(d => d.pageContent).join("\n---\n");
  const prompt = `
  Tu es un Ã©rudit de Rabbi Na'hman. RÃ©ponds en 4 parties numÃ©rotÃ©es :
  1. RÃ©ponse dÃ©taillÃ©e
  2. RÃ©fÃ©rence exacte (Livre Â§)
  3. Fil conducteur spirituel
  4. Conseils pratiques
  Utilise uniquement le CONTEXTE suivant :
  """${context}"""
  QUESTION : ${question}
  `;
  const model = genAI.getGenerativeModel({ model: "gemini-1.5-pro-latest" });
  const result = await model.generateContent({ contents:[{role:"user",parts:[{text:prompt}]}],
                                               generationConfig:{maxOutputTokens:2048,
                                                                 temperature:0.3}});
  res.json({ answer: result.response.text().trim() });
});
```

### 3. VÃ©rifications :

```bash
curl -X POST http://localhost:3000/ask \
     -H "Content-Type: application/json" \
     -d '{"question":"OÃ¹ RabbÃ©nou parle-t-il de la musique comme remÃ¨de ?"}'
```

â†’ Doit citer Â« LikoutÃ© Moharan 237 Â».

### 4. Surveillance auto

```bash
fswatch -o ./data/texts | xargs -n1 -I{} npm run ingest
```

---

## C Â· Booster la profondeur de rÃ©ponse

| Levier                                | Valeur conseillÃ©e | Effet                           |
| ------------------------------------- | ----------------- | ------------------------------- |
| `similaritySearch(q, 10â€“12)`          | 10 Ã  12 chunks    | + de contexte riche             |
| `maxOutputTokens`                     | 2048 (Gemini Pro) | RÃ©ponse longue                  |
| `temperature`                         | 0.3â€“0.4           | CohÃ©rence                       |
| Prompt â€œ4 parties numÃ©rotÃ©esâ€         | impose structure  | Lecture facile                  |
| *Streaming* (`generateContentStream`) | oui               | Affichage type machine-Ã -Ã©crire |

Front-end :

```tsx
for await (const chunk of stream) setAnswer(prev => prev + chunk.text);
```

---

## D Â· STT + TTS bidirectionnel

### 1. Reconnaissance vocale (STT)

```tsx
// hooks/useSpeech.ts
import { useEffect, useState } from "react";
export const useSpeech = (send: (txt:string)=>void) => {
  const [rec, setRec] = useState<SpeechRecognition|null>(null);
  useEffect(() => {
    const SR = (window as any).webkitSpeechRecognition || SpeechRecognition;
    const r = new SR(); r.lang = "fr-FR"; r.interimResults = false;
    r.onresult = e => {
      const t = Array.from(e.results).map(r=>r[0].transcript).join(" ");
      send(t);                       // 1. envoie dans lâ€™input
    };
    r.onerror = console.error;
    setRec(r);
  }, []);
  return () => rec?.start();
};
```

### 2. SynthÃ¨se vocale (TTS)

```ts
export const speak = (txt:string) => {
  const u = new SpeechSynthesisUtterance(txt);
  u.lang = "fr-FR";
  u.rate = 1.03; u.pitch = 0.9;
  speechSynthesis.cancel();
  speechSynthesis.speak(u);
};
```

### 3. IntÃ©gration UI

```tsx
const startRec = useSpeech(handleSend);
â€¦
<Button onClick={startRec}>ğŸ¤</Button>
â€¦
useEffect(()=>{ if(answer) speak(answer); },[answer]);
```

### 4. Fallback Whisper (si navigateur â‰  Chrome)

* Endpoint `/stt` qui reÃ§oit `audio/webm` â†’ envoie Ã  `speech-to-text-002`.
* Renvoie `{ transcript }` â†’ front appel `/ask`.

---

## E Â· Pipeline Git â†” Replit SANS Agent capricieux

1. **DÃ©sactive lâ€™Auto-Agent** (Settings > General > *Disable Replit AI Agent*).
2. **Replit** : `Git` tab â†’ assure-toi quâ€™il pointe sur **ton** repo, pas un fork.
3. **Workflow** :

   ```bash
   git checkout main
   git pull --rebase origin main
   # codeâ€¦
   git add .
   git commit -m "feat: STT+TTS, kill context"
   git push
   ```
4. **Container restart** Ã  chaque merge :

   * Ajoute un `postinstall` dans `package.json` : `"postinstall": "npm run ingest"`
   * Replit dÃ©tecte â†’ redÃ©ploie.

---

## F Â· Checklist dÃ©pannage express

| SymptÃ´me                      | Diagnostic                        | RemÃ¨de                                     |
| ----------------------------- | --------------------------------- | ------------------------------------------ |
| Bloc bleu rÃ©apparaÃ®t          | JSON contient `context`           | Cherche `res.json({ context`               |
| RÃ©ponse courte                | `maxOutputTokens` trop bas        | 2048                                       |
| â€œAucune source trouvÃ©eâ€       | Vectordb vide                     | `SELECT count(*) FROM breslev_texts;`      |
| Micro muet                    | `getUserMedia` non accordÃ©        | VÃ©rifier permissions navigateur            |
| Parole non lue                | `speechSynthesis.speaking` bloquÃ© | `speechSynthesis.cancel()` avant `speak()` |
| Replit nâ€™applique pas le code | Agent actif ou cache              | `kill 1` + hard refresh                    |

---

### Commande unique de reconstruction si tout casse

```bash
rm -rf node_modules dist
npm install
npm run ingest
npm run dev        # ou Run bouton Replit
```

---

## Conclusion & prochaines briques

1. **Applique les trois verrous (A)** â†’ plus aucun cadre parasite.
2. **Lance `npm run ingest`** â†’ DB remplie (â‰ˆ 25 000 chunks).
3. **Teste `/ask`** â†’ la rÃ©ponse arrive, longue, structurÃ©e -> TTS la lit.
4. **Valide** â†’ on branchera ensuite :

   * Auth Supabase + favoris,
   * Timeline de progression dâ€™Ã©tude,
   * Export PDF des sessions,
   * Mode â€œlive shiourâ€ (WebRTC).

Je reste en alerte : dis-moi quel point cale encore, ou colle-moi la moindre trace dâ€™erreur et on la dÃ©gomme !

Na Naá¸¥ Naá¸¥ma Naá¸¥man MÃ©ouman ğŸŒŸ
